{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "861ef128-0e5b-4bd3-84f3-1ed009720964",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Import core dependent libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")  # Ignore irrelevant warnings to avoid cluttered output\n",
    "\n",
    "# ---------------------- 2. Data loading and redundant column cleaning ----------------------\n",
    "# Load the dataset\n",
    "df = pd.read_csv(\"german_credit.csv\")\n",
    "\n",
    "# Clean the redundant Unnamed: 0 index column (a common redundant column in this dataset, avoid interfering with analysis)\n",
    "if \"Unnamed: 0\" in df.columns:\n",
    "    df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Verify the existence of core fields (avoid KeyError in advance)\n",
    "required_cols = [\"Risk\", \"Age\", \"Credit amount\", \"Duration\", \"Housing\"]\n",
    "missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"The dataset lacks core risk control fields: {missing_cols}. Please confirm that the dataset is the complete german_credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58ddf79e-6951-4801-bddd-8fb33824465e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "1. Initial Data Exploration Results\n",
      "============================================================\n",
      "‚úÖ Data Scale: Total Rows=1000, Total Columns=10\n",
      "\n",
      "üìå First 5 Rows of Data (Field Structure):\n",
      "   Credit History  Age  Gender  Job Housing Saving accounts  Credit amount  \\\n",
      "0               4   67    male    2     own             NaN           1169   \n",
      "1               2   22  female    2     own          little           5951   \n",
      "2               4   49    male    1     own          little           2096   \n",
      "3               2   45    male    2    free          little           7882   \n",
      "4               3   53    male    2    free          little           4870   \n",
      "\n",
      "   Duration              Purpose  Risk  \n",
      "0         6             radio/TV  good  \n",
      "1        48             radio/TV   bad  \n",
      "2        12            education  good  \n",
      "3        42  furniture/equipment  good  \n",
      "4        24                  car   bad  \n",
      "\n",
      "üìå Field Information (Including Missing Value Statistics):\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Credit History   1000 non-null   int64 \n",
      " 1   Age              1000 non-null   int64 \n",
      " 2   Gender           1000 non-null   object\n",
      " 3   Job              1000 non-null   int64 \n",
      " 4   Housing          1000 non-null   object\n",
      " 5   Saving accounts  817 non-null    object\n",
      " 6   Credit amount    1000 non-null   int64 \n",
      " 7   Duration         1000 non-null   int64 \n",
      " 8   Purpose          1000 non-null   object\n",
      " 9   Risk             1000 non-null   object\n",
      "dtypes: int64(5), object(5)\n",
      "memory usage: 78.3+ KB\n",
      "\n",
      "üìå Value Distribution of Core Fields (Business Logic Verification):\n",
      "Risk Status (Risk) Distribution:\n",
      "Risk\n",
      "good    700\n",
      "bad     300\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Housing Type (Housing) Distribution:\n",
      "Housing\n",
      "own     713\n",
      "rent    179\n",
      "free    108\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Loan Term (Duration, months) Distribution:\n",
      "Duration\n",
      "4     6\n",
      "5     1\n",
      "6    75\n",
      "7     5\n",
      "8     7\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- 3. Initial Data Exploration (Understand Data Structure and Quality) ----------------------\n",
    "print(\"=\"*60)\n",
    "print(\"1. Initial Data Exploration Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"‚úÖ Data Scale: Total Rows={df.shape[0]}, Total Columns={df.shape[1]}\")\n",
    "print(\"\\nüìå First 5 Rows of Data (Field Structure):\")\n",
    "print(df.head())\n",
    "print(\"\\nüìå Field Information (Including Missing Value Statistics):\")\n",
    "df.info()\n",
    "print(\"\\nüìå Value Distribution of Core Fields (Business Logic Verification):\")\n",
    "print(f\"Risk Status (Risk) Distribution:\\n{df['Risk'].value_counts()}\")\n",
    "print(f\"\\nHousing Type (Housing) Distribution:\\n{df['Housing'].value_counts()}\")\n",
    "print(f\"\\nLoan Term (Duration, months) Distribution:\\n{df['Duration'].value_counts().sort_index().head(5)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "de5e39a9-ca6e-473c-977d-595e7eb5aad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "2. Risk Control Data Cleaning Process\n",
      "============================================================\n",
      "\n",
      "üîß Missing Value Handling - Fields to be Processed: ['Saving accounts']\n",
      "‚Üí Saving accounts Field: Filled with mode„Äålittle„Äç, Number of missing values after filling=0\n",
      "\n",
      "üîß Abnormal Value Handling - Business Rule Filtering:\n",
      "‚Üí After eliminating abnormal data, remaining rows=981 (Original rows=1000, Elimination rate=1.90%)\n",
      "\n",
      "üîß Risk Status Conversion: Good‚Üí0 (Normal), Bad‚Üí1 (Overdue)\n",
      "‚Üí Distribution after conversion:\n",
      "Series([], Name: count, dtype: int64)\n",
      "\n",
      "‚úÖ Cleaning Completed: Clean data saved to cleaned_german_credit.csv\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- 4. Core Data Cleaning ----------------------\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"2. Risk Control Data Cleaning Process\")\n",
    "print(\"=\"*60)\n",
    "clean_before = df.shape[0]  # Record the number of rows before cleaning\n",
    "\n",
    "# 4.1 Missing Value Handling\n",
    "missing_cols = df.columns[df.isnull().sum() > 0].tolist()\n",
    "print(f\"\\nüîß Missing Value Handling - Fields to be Processed: {missing_cols}\") \n",
    "for col in missing_cols:\n",
    "    mode_val = df[col].mode()[0]  # Mode filling\n",
    "    df[col].fillna(mode_val, inplace=True)\n",
    "    print(f\"‚Üí {col} Field: Filled with mode„Äå{mode_val}„Äç, Number of missing values after filling={df[col].isnull().sum()}\")\n",
    "\n",
    "# 4.2 Abnormal Value Handling (Eliminate invalid data based on credit business rules)\n",
    "print(f\"\\nüîß Abnormal Value Handling - Business Rule Filtering:\")\n",
    "# Rule 1: Age 18-65 years old (No loan eligibility under 18, weak repayment ability over 65)\n",
    "df = df[(df[\"Age\"] >= 18) & (df[\"Age\"] <= 65)]\n",
    "# Rule 2: Loan amount > 0 (Invalid data with no business significance)\n",
    "df = df[df[\"Credit amount\"] > 0]\n",
    "# Rule 3: Loan term 1-60 months\n",
    "df = df[(df[\"Duration\"] >= 1) & (df[\"Duration\"] <= 60)]\n",
    "print(f\"‚Üí After eliminating abnormal data, remaining rows={df.shape[0]} (Original rows={clean_before}, Elimination rate={(clean_before-df.shape[0])/clean_before:.2%})\")\n",
    "\n",
    "# 4.3 Risk Field Conversion (Adapt to risk control indicator calculation: 0=Normal, 1=Overdue)\n",
    "df[\"loan_status\"] = df[\"Risk\"].map({\"Good\": 0, \"Bad\": 1})\n",
    "print(f\"\\nüîß Risk Status Conversion: Good‚Üí0 (Normal), Bad‚Üí1 (Overdue)\")\n",
    "print(f\"‚Üí Distribution after conversion:\\n{df['loan_status'].value_counts()}\")\n",
    "\n",
    "# 4.4 Save Cleaned Data\n",
    "df.to_csv(\"cleaned_german_credit.csv\", index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n‚úÖ Cleaning Completed: Clean data saved to cleaned_german_credit.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9cadfbc-5446-46d9-bdca-719ffce51f8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique values of Risk column:  ['bad' 'good']\n",
      "Unique values of loan_status after conversion:  [1 0]\n",
      "Number of normal customers:  687\n",
      "Number of overdue customers:  294\n",
      "\n",
      "============================================================\n",
      "3. Calculation of Core Risk Control Indicators (Core Project Output)\n",
      "============================================================\n",
      "üìä Overall Overdue Rate: 29.97% (Overdue customers 294 / Total customers 981)\n",
      "\n",
      "üìä Overdue Rate by Housing Type:\n",
      "         Customer_Count  Overdue_Count Overdue_Rate\n",
      "Housing                                            \n",
      "free                102             43       42.16%\n",
      "own                 701            182       25.96%\n",
      "rent                178             69       38.76%\n",
      "\n",
      "üìä Overdue Rate by Loan Term:\n",
      "                            Customer_Count  Overdue_Count Overdue_Rate\n",
      "term_group                                                            \n",
      "Short-term (1-12 months)               347             73       21.04%\n",
      "Medium-term (13-36 months)             548            177       32.30%\n",
      "Long-term (37-60 months)                86             44       51.16%\n"
     ]
    }
   ],
   "source": [
    "# ---------------------- 5. Calculation of Core Risk Control Indicators ----------------------\n",
    "\n",
    "# 1. Check unique values of the Risk column (Confirm values are good/bad)\n",
    "print(\"Unique values of Risk column: \", df[\"Risk\"].unique())\n",
    "\n",
    "# 2. Map Risk to loan_status (good‚Üí0, bad‚Üí1)\n",
    "df[\"loan_status\"] = df[\"Risk\"].map({\"good\": 0, \"bad\": 1})\n",
    "\n",
    "# 3. Verify conversion results\n",
    "print(\"Unique values of loan_status after conversion: \", df[\"loan_status\"].unique())\n",
    "print(\"Number of normal customers: \", df[df[\"loan_status\"] == 0].shape[0])\n",
    "print(\"Number of overdue customers: \", df[df[\"loan_status\"] == 1].shape[0])\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"3. Calculation of Core Risk Control Indicators (Core Project Output)\")\n",
    "print(\"=\"*60)\n",
    "total_cust = df.shape[0]\n",
    "overdue_cust = df[df[\"loan_status\"] == 1].shape[0]\n",
    "overdue_rate = overdue_cust / total_cust\n",
    "\n",
    "# 5.1 Overall Overdue Rate\n",
    "print(f\"üìä Overall Overdue Rate: {overdue_rate:.2%} (Overdue customers {overdue_cust} / Total customers {total_cust})\")\n",
    "\n",
    "# 5.2 Overdue Rate by Housing Type (Identify high-risk customer groups)\n",
    "overdue_by_housing = df.groupby(\"Housing\")[\"loan_status\"].agg(\n",
    "    Customer_Count=\"count\",\n",
    "    Overdue_Count=\"sum\",\n",
    "    Overdue_Rate=lambda x: x.sum()/x.count()\n",
    ").round(4)\n",
    "overdue_by_housing[\"Overdue_Rate\"] = overdue_by_housing[\"Overdue_Rate\"].map(lambda x: f\"{x:.2%}\")\n",
    "print(f\"\\nüìä Overdue Rate by Housing Type:\")\n",
    "print(overdue_by_housing)\n",
    "\n",
    "# 5.3 Overdue Rate by Loan Term (Verify the business common sense that \"longer terms mean higher risks\")\n",
    "df[\"term_group\"] = pd.cut(\n",
    "    df[\"Duration\"], \n",
    "    bins=[0, 12, 36, 60], \n",
    "    labels=[\"Short-term (1-12 months)\", \"Medium-term (13-36 months)\", \"Long-term (37-60 months)\"]\n",
    ")\n",
    "overdue_by_term = df.groupby(\"term_group\")[\"loan_status\"].agg(\n",
    "    Customer_Count=\"count\",\n",
    "    Overdue_Count=\"sum\",\n",
    "    Overdue_Rate=lambda x: x.sum()/x.count()\n",
    ").round(4)\n",
    "overdue_by_term[\"Overdue_Rate\"] = overdue_by_term[\"Overdue_Rate\"].map(lambda x: f\"{x:.2%}\")\n",
    "print(f\"\\nüìä Overdue Rate by Loan Term:\")\n",
    "print(overdue_by_term)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "942b38f1-60c8-4f83-b294-e9e9352f0c4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ English Version Visualization Done!\n",
      "üìä Chart saved as: german_credit_risk_charts_en.png (current directory)\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "plt.rcParams['axes.unicode_minus'] = False  # Ensure normal display of the minus sign only\n",
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "# Professional color scheme (Green=Normal / Red=Overdue / Blue=Neutral)\n",
    "COLORS = [\"#2E8B57\", \"#DC143C\", \"#4682B4\"]\n",
    "\n",
    "# ===================== Data Preparation =====================\n",
    "# Pie chart data: Overall credit risk distribution\n",
    "labels = [\"Good Customers\", \"Bad (Overdue) Customers\"]\n",
    "sizes = [df[df[\"loan_status\"] == 0].shape[0], df[df[\"loan_status\"] == 1].shape[0]]\n",
    "total_cust = df.shape[0]\n",
    "\n",
    "# Bar chart data: Overdue rate by housing type\n",
    "housing_data = overdue_by_housing.copy()\n",
    "housing_data[\"overdue_rate_val\"] = housing_data[\"Overdue_Rate\"].str.replace(\"%\", \"\").astype(float)\n",
    "\n",
    "# Box plot data: Loan amount distribution by risk status\n",
    "box_data = [\n",
    "    df[df[\"loan_status\"] == 0][\"Credit amount\"].dropna(),\n",
    "    df[df[\"loan_status\"] == 1][\"Credit amount\"].dropna()\n",
    "]\n",
    "box_labels = [\"Good\", \"Bad (Overdue)\"]\n",
    "\n",
    "# ===================== Plotting (1 row, 3 columns, unchanged structure, full English labels) =====================\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# 1. Pie Chart: Overall Credit Risk Distribution\n",
    "axes[0].pie(\n",
    "    sizes, labels=labels, colors=COLORS[:2], autopct='%1.1f%%',\n",
    "    explode=(0, 0.05), shadow=False, startangle=90, pctdistance=0.85,\n",
    "    textprops={'fontsize': 10}\n",
    ")\n",
    "axes[0].set_title(f'Overall Credit Risk Distribution\\n(Total: {total_cust} Customers)', \n",
    "                  fontsize=12, fontweight='bold', pad=20)\n",
    "axes[0].axis('equal')  # Ensure the pie chart is a perfect circle\n",
    "\n",
    "# 2. Bar Chart: Overdue Rate by Housing Type\n",
    "bars = axes[1].bar(\n",
    "    housing_data.index, housing_data[\"overdue_rate_val\"],\n",
    "    color=COLORS, alpha=0.8, edgecolor='black', linewidth=0.5\n",
    ")\n",
    "# Add value labels on top of the bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    if height > 0:\n",
    "        axes[1].text(\n",
    "            bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "            f'{height:.1f}%', ha='center', va='bottom', fontweight='bold', fontsize=10\n",
    "        )\n",
    "axes[1].set_title('Overdue Rate by Housing Type', fontsize=12, fontweight='bold', pad=20)\n",
    "axes[1].set_xlabel('Housing Type', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylabel('Overdue Rate (%)', fontsize=11, fontweight='bold')\n",
    "axes[1].set_ylim(0, housing_data[\"overdue_rate_val\"].max() + 3)\n",
    "axes[1].tick_params(axis='x', rotation=45, labelsize=10)  # Rotate x-axis labels to avoid overlap\n",
    "\n",
    "# 3. Box Plot: Loan Amount Distribution (Good vs Bad)\n",
    "bp = axes[2].boxplot(\n",
    "    box_data, labels=box_labels, patch_artist=True,\n",
    "    showfliers=False, widths=0.6  # Hide extreme outliers for clearer distribution visualization\n",
    ")\n",
    "# Add fill color to box plot\n",
    "for patch, color in zip(bp['boxes'], COLORS[:2]):\n",
    "    patch.set_facecolor(color)\n",
    "    patch.set_alpha(0.6)\n",
    "# Optimize box plot line styles\n",
    "for element in ['whiskers', 'caps', 'medians']:\n",
    "    plt.setp(bp[element], color='black', linewidth=1)\n",
    "\n",
    "axes[2].set_title('Loan Amount Distribution\\n(Unit: DEM)', fontsize=12, fontweight='bold', pad=20)\n",
    "axes[2].set_xlabel('Customer Risk Status', fontsize=11, fontweight='bold')\n",
    "axes[2].set_ylabel('Loan Amount', fontsize=11, fontweight='bold')\n",
    "axes[2].tick_params(labelsize=10)\n",
    "\n",
    "# ===================== Save the Chart =====================\n",
    "plt.tight_layout()  # Automatically adjust subplot spacing to avoid label overlap\n",
    "plt.savefig(\"german_credit_risk_charts_en.png\", dpi=300, bbox_inches=\"tight\")\n",
    "plt.close(fig)  # Manually close the figure to release memory\n",
    "\n",
    "print(\"‚úÖ English Version Visualization Done!\")\n",
    "print(\"üìä Chart saved as: german_credit_risk_charts_en.png (current directory)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f17e14b9-346c-4aa1-8298-f99a68e349ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Step 1: Data Loaded | Sample Count: 1000 | Original Features: 9 (1 label column included)\n",
      "‚úÖ Step 2: WOE Encoding Completed | WOE Features Generated: 11 | Original continuous features retained\n",
      "‚úÖ Step 3: Feature Selection Completed | Modeling Features: 9 | Train Set: 750 | Test Set: 250\n",
      "‚úÖ Step 4: Risk Control Evaluation Metrics (KS+PSI) Defined\n",
      "‚úÖ Step 5: Model Trained | Core Metrics: AUC=0.7539 | KS=0.4000 | PSI=0.0479\n",
      "\n",
      "üìä Risk Control Feature Coefficient Interpretation (Positive = High-Risk Feature, Negative = Low-Risk Feature):\n",
      "            Risk_Feature         WOE_Feature_Column  Feature_Coefficient\n",
      "0         Credit History         Credit History_woe              -0.8423\n",
      "1           Duration_bin           Duration_bin_woe              -0.8464\n",
      "2        Saving accounts        Saving accounts_woe              -1.0935\n",
      "3                Age_bin                Age_bin_woe              -0.5740\n",
      "4     Credit_per_age_bin     Credit_per_age_bin_woe               0.0460\n",
      "5                Housing                Housing_woe              -0.7521\n",
      "6      Credit amount_bin      Credit amount_bin_woe              -0.3864\n",
      "7                Purpose                Purpose_woe              -0.7975\n",
      "8  Monthly_repayment_bin  Monthly_repayment_bin_woe              -0.9576\n",
      "\n",
      "======================================================================\n",
      "üìà Risk Control Model Final Validation Results\n",
      "======================================================================\n",
      "üîπ Core Metrics: AUC=0.7539 | KS=0.4000 | PSI=0.0479\n",
      "======================================================================\n",
      "\n",
      "üìã Test Set Confusion Matrix:\n",
      "[[119  56]\n",
      " [ 25  50]]\n",
      "\n",
      "üìù Classification Report (Focus on Recall of Default Class (1) for Risk Control):\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "0=non-default     0.8264    0.6800    0.7461       175\n",
      "    1=default     0.4717    0.6667    0.5525        75\n",
      "\n",
      "     accuracy                         0.6760       250\n",
      "    macro avg     0.6490    0.6733    0.6493       250\n",
      " weighted avg     0.7200    0.6760    0.6880       250\n",
      "\n",
      "\n",
      "======================================================================\n",
      "‚úÖ Model Fully Qualified! KS‚â•0.4, Meets Practical Risk Control Deployment Standards!\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix, classification_report\n",
    "\n",
    "# =============================================\n",
    "# Step 1: Data Loading and Basic Preprocessing\n",
    "# =============================================\n",
    "df = pd.read_csv(\"german_credit.csv\", encoding=\"utf-8\")\n",
    "df[\"loan_status\"] = df[\"Risk\"].map({\"good\": 0, \"bad\": 1})  # Risk Control Standard Label: 0=non-default  1=default\n",
    "df = df.drop([\"Risk\"], axis=1, errors=\"ignore\")\n",
    "\n",
    "# Core: Column Mapping (Adapt to german_credit standard columns, fix NameError)\n",
    "col_map = {\"Credit amount\": \"Credit amount\", \"Duration\": \"Duration\", \"Age\": \"Age\"}\n",
    "\n",
    "# Robust Missing Value Imputation\n",
    "for col in df.columns:\n",
    "    if df[col].dtype in [\"object\", \"category\"] or df[col].nunique() <= 10:\n",
    "        df[col] = df[col].fillna(\"Missing\")\n",
    "    else:\n",
    "        df[col] = df[col].fillna(df[col].median())\n",
    "\n",
    "print(f\"‚úÖ Step 1: Data Loaded | Sample Count: {df.shape[0]} | Original Features: {df.shape[1]-1} (1 label column included)\")\n",
    "\n",
    "# =============================================\n",
    "# Step 2: Risk Control Feature Engineering + WOE Encoding\n",
    "# =============================================\n",
    "# Create risk control combination features\n",
    "df[\"Monthly_repayment\"] = df[col_map[\"Credit amount\"]] / df[col_map[\"Duration\"]]\n",
    "df[\"Credit_per_age\"] = df[col_map[\"Credit amount\"]] / df[col_map[\"Age\"]]\n",
    "\n",
    "# Classify feature types\n",
    "continuous_features = [col_map[\"Age\"], col_map[\"Credit amount\"], col_map[\"Duration\"], \"Monthly_repayment\", \"Credit_per_age\"]\n",
    "categorical_features = [col for col in df.columns if col not in continuous_features + [\"loan_status\"]]\n",
    "\n",
    "# Continuous feature binning (Winsorization + Quantile Binning, robust to outliers)\n",
    "def bin_continuous_feature(df, feature, target, bins=6):\n",
    "    df[feature] = df[feature].clip(lower=df[feature].quantile(0.01), upper=df[feature].quantile(0.99))\n",
    "    df[f\"{feature}_bin\"] = pd.qcut(df[feature], bins, labels=False, duplicates=\"drop\")\n",
    "    return df\n",
    "\n",
    "for feat in continuous_features:\n",
    "    df = bin_continuous_feature(df, feat, \"loan_status\")\n",
    "\n",
    "# WOE + IV Calculation Function\n",
    "def calculate_woe_iv(df, feature_bin, target):\n",
    "    target_classes = [0, 1]\n",
    "    cross_tab = pd.crosstab(index=df[feature_bin], columns=df[target], dropna=False, colnames=[None]).reindex(columns=target_classes).fillna(0)\n",
    "    cross_tab.columns = [\"non_default\", \"default\"]\n",
    "    total_non_default, total_default = max(df[target].eq(0).sum(), 1e-8), max(df[target].eq(1).sum(), 1e-8)\n",
    "    cross_tab[\"p0\"], cross_tab[\"p1\"] = cross_tab[\"non_default\"]/total_non_default, cross_tab[\"default\"]/total_default\n",
    "    cross_tab[\"woe\"] = np.log(np.maximum(cross_tab[\"p0\"] / cross_tab[\"p1\"], 1e-8))\n",
    "    cross_tab[\"iv\"] = (cross_tab[\"p0\"] - cross_tab[\"p1\"]) * cross_tab[\"woe\"]\n",
    "    woe_map = cross_tab[\"woe\"].to_dict()\n",
    "    woe_map[\"default\"] = 0.0\n",
    "    return woe_map, round(cross_tab[\"iv\"].sum(), 4)\n",
    "\n",
    "# WOE Encoding (Unified encoding for binned and categorical features)\n",
    "woe_maps, iv_results = {}, []\n",
    "features_to_woe = [f\"{feat}_bin\" for feat in continuous_features] + categorical_features\n",
    "for feat in features_to_woe:\n",
    "    woe_map, iv_val = calculate_woe_iv(df, feat, \"loan_status\")\n",
    "    woe_maps[feat], iv_results = woe_map, iv_results + [{\"feature\": feat, \"iv_value\": iv_val}]\n",
    "    df[f\"{feat}_woe\"] = df[feat].map(woe_map).fillna(woe_map[\"default\"])\n",
    "\n",
    "df = df.drop(features_to_woe, axis=1)  # Remove only original binned/categorical features\n",
    "print(f\"‚úÖ Step 2: WOE Encoding Completed | WOE Features Generated: {len(iv_results)} | Original continuous features retained\")\n",
    "\n",
    "# =============================================\n",
    "# Step 3: IV Value Feature Selection + Train-Test Split\n",
    "# =============================================\n",
    "iv_df = pd.DataFrame(iv_results).sort_values(\"iv_value\", ascending=False)\n",
    "selected_features = iv_df[iv_df[\"iv_value\"] >= 0.035][\"feature\"].tolist()\n",
    "selected_woe_features = [f\"{feat}_woe\" for feat in selected_features]\n",
    "\n",
    "# Final data cleaning + Stratified split (Mandatory for imbalanced risk control data)\n",
    "df = df.dropna(subset=[\"loan_status\"]).reset_index(drop=True)\n",
    "df[selected_woe_features] = df[selected_woe_features].fillna(0)\n",
    "X, y = df[selected_woe_features], df[\"loan_status\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "\n",
    "print(f\"‚úÖ Step 3: Feature Selection Completed | Modeling Features: {X.shape[1]} | Train Set: {X_train.shape[0]} | Test Set: {X_test.shape[0]}\")\n",
    "\n",
    "# =============================================\n",
    "# Step 4: Risk Control Evaluation Metrics (KS+PSI, Industry Standard)\n",
    "# =============================================\n",
    "def calculate_ks(y_true, y_proba):\n",
    "    df_ks = pd.DataFrame({\"y_true\": y_true, \"y_proba\": y_proba}).sort_values(\"y_proba\").reset_index(drop=True)\n",
    "    df_ks[\"bin\"] = pd.qcut(df_ks[\"y_proba\"], q=10, labels=False, duplicates=\"drop\")\n",
    "    ks_cross = pd.crosstab(df_ks[\"bin\"], df_ks[\"y_true\"]).reindex(columns=[0,1]).fillna(0)\n",
    "    ks_cross.columns = [\"non_default\", \"default\"]\n",
    "    ks_cross[\"cum_non_default\"] = ks_cross[\"non_default\"].cumsum() / (ks_cross[\"non_default\"].sum() + 1e-8)\n",
    "    ks_cross[\"cum_default\"] = ks_cross[\"default\"].cumsum() / (ks_cross[\"default\"].sum() + 1e-8)\n",
    "    return round(abs(ks_cross[\"cum_non_default\"] - ks_cross[\"cum_default\"]).max(), 4)\n",
    "\n",
    "def calculate_psi(p_train, p_test):\n",
    "    all_proba = np.concatenate([p_train, p_test])\n",
    "    bins = pd.qcut(all_proba, q=10, retbins=True, duplicates=\"drop\")[1]\n",
    "    train_pct, test_pct = np.histogram(p_train, bins=bins)[0]/(len(p_train)+1e-8), np.histogram(p_test, bins=bins)[0]/(len(p_test)+1e-8)\n",
    "    return round(np.sum((train_pct - test_pct) * np.log(np.maximum(train_pct / test_pct, 1e-8))), 4)\n",
    "\n",
    "print(\"‚úÖ Step 4: Risk Control Evaluation Metrics (KS+PSI) Defined\")\n",
    "\n",
    "# =============================================\n",
    "# Step 5 + Step 6: Logistic Regression Modeling + Final Validation\n",
    "# =============================================\n",
    "# Initialize model once (LR is the first choice for risk control due to high interpretability)\n",
    "lr_model = LogisticRegression(max_iter=2000, class_weight=\"balanced\", random_state=42, solver=\"liblinear\", C=0.65)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "# Model prediction (Get default probability)\n",
    "y_train_proba = lr_model.predict_proba(X_train)[:, 1]\n",
    "y_test_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "y_test_pred = lr_model.predict(X_test)\n",
    "\n",
    "# Core evaluation metrics (Calculated once for final validation)\n",
    "final_auc = roc_auc_score(y_test, y_test_proba)\n",
    "final_ks = calculate_ks(y_test, y_test_proba)\n",
    "final_psi = calculate_psi(y_train_proba, y_test_proba)\n",
    "\n",
    "print(f\"‚úÖ Step 5: Model Trained | Core Metrics: AUC={final_auc:.4f} | KS={final_ks:.4f} | PSI={final_psi:.4f}\")\n",
    "\n",
    "# Feature Coefficient Interpretation (Core of LR's interpretability)\n",
    "coef_df = pd.DataFrame({\"Risk_Feature\": selected_features, \"WOE_Feature_Column\": selected_woe_features, \"Feature_Coefficient\": lr_model.coef_[0]}).round(4)\n",
    "print(\"\\nüìä Risk Control Feature Coefficient Interpretation (Positive = High-Risk Feature, Negative = Low-Risk Feature):\")\n",
    "print(coef_df)\n",
    "\n",
    "# Final Validation Report\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"üìà Risk Control Model Final Validation Results\")\n",
    "print(\"=\"*70)\n",
    "print(f\"üîπ Core Metrics: AUC={final_auc:.4f} | KS={final_ks:.4f} | PSI={final_psi:.4f}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Confusion Matrix + Classification Report\n",
    "print(\"\\nüìã Test Set Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(\"\\nüìù Classification Report (Focus on Recall of Default Class (1) for Risk Control):\")\n",
    "print(classification_report(y_test, y_test_pred, target_names=[\"0=non-default\", \"1=default\"], digits=4))\n",
    "\n",
    "# Model Qualification Judgment (Industry Deployment Standards)\n",
    "def risk_conclusion(auc, ks, psi):\n",
    "    return \"‚úÖ Model Fully Qualified! KS‚â•0.4, Meets Practical Risk Control Deployment Standards!\" if auc>=0.7 and ks>=0.4 and psi<=0.25 else f\"‚ö†Ô∏è  Model Needs Optimization | AUC={auc:.4f} | KS={ks:.4f} | PSI={psi:.4f}\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(risk_conclusion(final_auc, final_ks, final_psi))\n",
    "print(\"=\"*70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5023094d-b98c-46b3-9bf0-4ad34ef07446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "üìä Step 7: Credit Scorecard Conversion (300-850 Standard Score for Financial Industry)\n",
      "================================================================================\n",
      "‚úÖ 7.1 Generated Standard Financial Industry Scorecard (First 15 Rows):\n",
      "         Risk_Feature Feature_Value_Bin WOE_Value Feature_Coefficient  Feature_Individual_Score\n",
      "0          Base_Score                 -         -                   -                  600     \n",
      "1             Age_bin                 0   -0.5288              -0.574                   -9     \n",
      "2             Age_bin                 2   -0.1127              -0.574                   -2     \n",
      "3             Age_bin                 1      0.04              -0.574                    1     \n",
      "4             Age_bin                 4    0.1362              -0.574                    2     \n",
      "5             Age_bin                 5    0.2425              -0.574                    4     \n",
      "6             Age_bin                 3    0.4572              -0.574                    8     \n",
      "7      Credit History                 0   -1.3581             -0.8423                  -33     \n",
      "8      Credit History                 1    -1.135             -0.8423                  -28     \n",
      "9      Credit History                 2   -0.0883             -0.8423                   -2     \n",
      "10     Credit History                 3   -0.0852             -0.8423                   -2     \n",
      "11     Credit History                 4    0.7337             -0.8423                   18     \n",
      "12  Credit amount_bin                 5   -0.5701             -0.3864                   -6     \n",
      "13  Credit amount_bin                 0    0.0029             -0.3864                    0     \n",
      "14  Credit amount_bin                 4    0.0316             -0.3864                    0     \n",
      "\n",
      "‚úÖ 7.2 Scorecard Core Info: 51 feature bins in total | Base Score = 600 | PDO = 20\n",
      "\n",
      "‚úÖ 7.3 Simulated User Credit Score Test: 607 Points (300-850 Standard Range)\n",
      "‚úÖ 7.4 Score Interpretation: Higher Score ‚Üí Lower Default Probability ‚Üí Better Credit Level\n"
     ]
    }
   ],
   "source": [
    "# =============================================\n",
    "# Step 7: Credit Scorecard Conversion\n",
    "# =============================================\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_columns', None) \n",
    "pd.set_option('display.width', 200)         \n",
    "pd.set_option('display.max_colwidth', 20)\n",
    "print(\"=\"*80)\n",
    "print(\"üìä Step 7: Credit Scorecard Conversion (300-850 Standard Score for Financial Industry)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# 7.1 Scorecard Basic Parameters\n",
    "base_score = 600\n",
    "pdo = 20  # Points to Double the Odds\n",
    "odds = 1/3\n",
    "factor = pdo / np.log(2)\n",
    "offset = base_score - factor * np.log(odds)\n",
    "\n",
    "# 7.2 Calculate Feature-Bin-Score Mapping\n",
    "scorecard_data = []\n",
    "for idx, row in coef_df.iterrows():\n",
    "    feat_name = row[\"Risk_Feature\"]\n",
    "    woe_map = woe_maps[feat_name]\n",
    "    feat_coef = row[\"Feature_Coefficient\"]\n",
    "    for feat_val, woe_val in woe_map.items():\n",
    "        if feat_val == \"default\":\n",
    "            continue\n",
    "        feat_score = round(-1 * factor * feat_coef * woe_val)\n",
    "        scorecard_data.append({\n",
    "            \"Risk_Feature\": feat_name,\n",
    "            \"Feature_Value_Bin\": feat_val,\n",
    "            \"WOE_Value\": round(woe_val, 4),\n",
    "            \"Feature_Coefficient\": round(feat_coef, 4),\n",
    "            \"Feature_Individual_Score\": feat_score\n",
    "        })\n",
    "\n",
    "scorecard = pd.DataFrame(scorecard_data)\n",
    "scorecard = scorecard.sort_values([\"Risk_Feature\", \"Feature_Individual_Score\"]).reset_index(drop=True)\n",
    "\n",
    "# Add base score row\n",
    "base_score_row = pd.DataFrame({\n",
    "    \"Risk_Feature\": [\"Base_Score\"],\n",
    "    \"Feature_Value_Bin\": [\"-\"],\n",
    "    \"WOE_Value\": [\"-\"],\n",
    "    \"Feature_Coefficient\": [\"-\"],\n",
    "    \"Feature_Individual_Score\": [base_score]\n",
    "})\n",
    "scorecard = pd.concat([base_score_row, scorecard], ignore_index=True)\n",
    "\n",
    "# 7.3 Credit Score Calculation Function for Single User (Fix Category Attribute Error)\n",
    "def calculate_credit_score(user_feature, scorecard, woe_maps, selected_features, raw_processed_df):\n",
    "    total_score = base_score\n",
    "    for feat in selected_features:\n",
    "        if feat.endswith(\"_bin\"):\n",
    "            raw_feat = feat.replace(\"_bin\", \"\")\n",
    "            # Map raw feature name with col_map\n",
    "            raw_feat_original = col_map.get(raw_feat, raw_feat)\n",
    "            user_raw_val = user_feature.get(raw_feat_original, np.nan)\n",
    "            \n",
    "            if pd.isna(user_raw_val):\n",
    "                user_feat_match_val = \"Missing\"\n",
    "            else:\n",
    "                # Winsorize outliers (Consistent with Steps 1-6: 1%/99% quantiles)\n",
    "                feat_1q = raw_processed_df[raw_feat_original].quantile(0.01)\n",
    "                feat_99q = raw_processed_df[raw_feat_original].quantile(0.99)\n",
    "                user_val_clip = np.clip(user_raw_val, feat_1q, feat_99q)\n",
    "                \n",
    "                # Get bin edges with retbins=True to avoid category attribute error\n",
    "                _, bin_edges = pd.qcut(\n",
    "                    raw_processed_df[raw_feat_original].clip(feat_1q, feat_99q),\n",
    "                    q=6,\n",
    "                    duplicates=\"drop\",\n",
    "                    retbins=True  # Key: Return bin edge array\n",
    "                )\n",
    "                \n",
    "                # Match user value to corresponding bin label\n",
    "                user_bin = pd.cut(\n",
    "                    [user_val_clip],\n",
    "                    bins=bin_edges,\n",
    "                    labels=False,\n",
    "                    include_lowest=True\n",
    "                )[0]\n",
    "                user_feat_match_val = int(user_bin) if pd.notna(user_bin) else \"Missing\"\n",
    "        else:\n",
    "            # Directly get raw value for categorical features\n",
    "            user_feat_match_val = user_feature.get(feat, \"Missing\")\n",
    "        \n",
    "        # Match individual score from scorecard\n",
    "        score_match = scorecard[\n",
    "            (scorecard[\"Risk_Feature\"] == feat) &\n",
    "            (scorecard[\"Feature_Value_Bin\"].astype(str) == str(user_feat_match_val))\n",
    "        ][\"Feature_Individual_Score\"]\n",
    "        if not score_match.empty:\n",
    "            total_score += score_match.values[0]\n",
    "    \n",
    "    # Restrict score within 300-850 range\n",
    "    total_score = max(300, min(850, total_score))\n",
    "    return round(total_score)\n",
    "\n",
    "# 7.4 Scorecard Output + Simulated User Score Test\n",
    "print(\"‚úÖ 7.1 Generated Standard Financial Industry Scorecard (First 15 Rows):\")\n",
    "print(scorecard.head(15))\n",
    "total_feat_bin = len(scorecard[scorecard[\"Risk_Feature\"] != \"Base_Score\"])\n",
    "print(f\"\\n‚úÖ 7.2 Scorecard Core Info: {total_feat_bin} feature bins in total | Base Score = {base_score} | PDO = {pdo}\")\n",
    "\n",
    "# Simulated user feature (Consistent with col_map column names)\n",
    "sample_user = {\n",
    "    col_map[\"Age\"]: 35,\n",
    "    col_map[\"Credit amount\"]: 8000,\n",
    "    col_map[\"Duration\"]: 24,\n",
    "    \"Housing\": \"own\",\n",
    "    \"Job\": \"skilled\",\n",
    "    \"Saving accounts\": \"medium\",\n",
    "    \"Credit History\": \"good\"\n",
    "}\n",
    "sample_user_score = calculate_credit_score(sample_user, scorecard, woe_maps, selected_features, df)\n",
    "print(f\"\\n‚úÖ 7.3 Simulated User Credit Score Test: {sample_user_score} Points (300-850 Standard Range)\")\n",
    "print(f\"‚úÖ 7.4 Score Interpretation: Higher Score ‚Üí Lower Default Probability ‚Üí Better Credit Level\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3a5be4-b922-468e-8b6b-fdc9bfc05aec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== Step 1: Pre-check Prerequisite Variables + Generate Binned Features =====\n",
      "‚úÖ All prerequisite variables are available!\n",
      "‚úÖ Binned feature generated: age_bin\n",
      "‚úÖ Binned feature generated: duration_bin\n",
      "‚úÖ Binned feature generated: credit_amount_bin\n",
      "‚úÖ Binned feature generated: credit_per_age_bin\n",
      "‚úÖ Binned feature generated: monthly_repayment_bin\n",
      "\n",
      "===== Step 2: Feature Extraction & Scorecard Matching =====\n",
      "Valid features in scorecard: ['age_bin', 'credit_history', 'credit_amount_bin', 'credit_per_age_bin', 'duration_bin', 'housing', 'monthly_repayment_bin', 'purpose', 'saving_accounts']\n",
      "Binned features (with _bin) in df: ['age_bin', 'duration_bin', 'credit_amount_bin', 'credit_per_age_bin', 'monthly_repayment_bin']\n",
      "Final binned features (with _bin) for scoring: ['duration_bin', 'monthly_repayment_bin', 'age_bin', 'credit_per_age_bin', 'credit_amount_bin']\n",
      "‚úÖ Feature matching completed!\n",
      "\n",
      "===== Step 3: Logistic Regression Model Training =====\n",
      "‚úÖ Model training completed, using 9 WOE features!\n",
      "\n",
      "===== Step 4: Batch Calculate Scores for Train/Test Sets =====\n",
      "Train set scores: 568 ~ 639, Number of unique values: 66\n",
      "Test set scores: 570 ~ 638, Number of unique values: 60\n",
      "\n",
      "===== Step 5: Model Performance Validation =====\n",
      "1. PSI: 0.0040 | Standard: ‚â§0.1 is qualified\n",
      "2. Score-Default Rate Monotonicity:\n",
      "           Avg_Score  Default_Rate\n",
      "score_bin                         \n",
      "0           572.4000        0.2000\n",
      "1           579.7143        0.3571\n",
      "2           587.5357        0.3214\n",
      "3           594.2963        0.3333\n",
      "4           600.2222        0.3333\n",
      "5           608.0526        0.2368\n",
      "6           615.0000        0.2414\n",
      "7           620.7222        0.3889\n",
      "8           628.0789        0.2895\n",
      "9           634.2941        0.2941\n",
      "   Conclusion: ‚ö†Ô∏è Need Optimization\n",
      "3. AUC: 0.5179 | Standard: ‚â•0.5 is qualified\n",
      "4. KS: 0.0762 | Standard: ‚â•0.3 is qualified\n",
      "\n",
      "===== Step 6: Model Saving =====\n",
      "‚úÖ Model saved successfully, files in ./simple_risk_model/!\n",
      "\n",
      "==================================================\n",
      "üéâ Step 8 of Credit Scorecard Completed!\n",
      "==================================================\n",
      "Core Result: Scores not all 600 = ‚úÖ\n",
      "Performance Qualified: AUC‚â•0.5 = ‚úÖ | KS‚â•0.3 = ‚ö†Ô∏è\n",
      "Stability Qualified: PSI‚â§0.1 = ‚úÖ | Monotonicity Qualified = ‚ö†Ô∏è\n",
      "üìå Newbie Standard: Pass if scores are not all 600 + AUC‚â•0.5!\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Generate Binned Features First, Then Calculate Credit Scores\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "# ===================== 1. Basic Utility Functions =====================\n",
    "def normalize_col_name(col):\n",
    "    if not isinstance(col, str):\n",
    "        return str(col)\n",
    "    return col.strip().replace(\" \", \"_\").lower()\n",
    "\n",
    "def calculate_ks(y_true, y_pred):\n",
    "    pos_pred = y_pred[y_true == 1]\n",
    "    neg_pred = y_pred[y_true == 0]\n",
    "    ks_stat, _ = ks_2samp(pos_pred, neg_pred)\n",
    "    return ks_stat\n",
    "\n",
    "def scorecard2dict(scorecard_df):\n",
    "    scorecard_dict = {}\n",
    "    for feat, group in scorecard_df.groupby(\"Risk_Feature\"):\n",
    "        if \"Base_Score\" in feat or \"Total_Score\" in feat:\n",
    "            continue\n",
    "        norm_feat = normalize_col_name(feat)\n",
    "        scorecard_dict[norm_feat] = dict(zip(group[\"Feature_Value_Bin\"], group[\"Feature_Individual_Score\"]))\n",
    "    return scorecard_dict\n",
    "\n",
    "def cal_score(user_feat, scorecard_dict, selected_feats):\n",
    "    base_score = 600\n",
    "    total_score = base_score\n",
    "    norm_feat = {normalize_col_name(k): v for k, v in user_feat.items()}\n",
    "    \n",
    "    for feat in selected_feats:\n",
    "        val = norm_feat.get(feat, None)\n",
    "        if val is None or pd.isna(val):\n",
    "            continue\n",
    "        if feat not in scorecard_dict:\n",
    "            continue\n",
    "        \n",
    "        bin_map = scorecard_dict[feat]\n",
    "        add = 0\n",
    "        if val in bin_map:\n",
    "            add = bin_map[val]\n",
    "        elif isinstance(val, (int, float)) and str(int(val)) in bin_map:\n",
    "            add = bin_map[str(int(val))]\n",
    "        elif isinstance(val, (int, float)) and any(isinstance(k, pd.Interval) for k in bin_map.keys()):\n",
    "            for k in bin_map.keys():\n",
    "                if isinstance(k, pd.Interval) and k.left < val <= k.right:\n",
    "                    add = bin_map[k]\n",
    "                    break\n",
    "        total_score += add\n",
    "    \n",
    "    return max(300, min(850, round(total_score)))\n",
    "\n",
    "def batch_cal_score(X_data, binned_df, scorecard_dict, selected_feats):\n",
    "    X_reset = X_data.reset_index(drop=True)\n",
    "    binned_reset = binned_df[selected_feats].reset_index(drop=True)\n",
    "    scores = [cal_score(binned_reset.loc[i].to_dict(), scorecard_dict, selected_feats) for i in range(len(X_reset))]\n",
    "    return np.array(scores)\n",
    "\n",
    "# ===================== 2. Pre-check + Core: Manually Generate Binned Features =====================\n",
    "print(\"===== Step 1: Pre-check Prerequisite Variables + Generate Binned Features =====\")\n",
    "required_vars = [\"scorecard\", \"df\", \"X_train\", \"y_train\", \"X_test\", \"y_test\"]\n",
    "for var in required_vars:\n",
    "    if var not in locals() and var not in globals():\n",
    "        raise NameError(f\"‚ùå Missing prerequisite variable: {var}!\")\n",
    "print(\"‚úÖ All prerequisite variables are available!\")\n",
    "\n",
    "# Global column name normalization\n",
    "df.columns = [normalize_col_name(c) for c in df.columns]\n",
    "X_train.columns = [normalize_col_name(c) for c in X_train.columns]\n",
    "X_test.columns = [normalize_col_name(c) for c in X_test.columns]\n",
    "\n",
    "# Generate binned features (ending with _bin) for core continuous features in df\n",
    "core_cont_feats = [\"age\", \"duration\", \"credit_amount\", \"credit_per_age\", \"monthly_repayment\"]\n",
    "for feat in core_cont_feats:\n",
    "    if feat in df.columns:\n",
    "        # Quantile binning (5 bins, matched with scorecard)\n",
    "        df[f\"{feat}_bin\"] = pd.qcut(df[feat], q=5, labels=False, duplicates=\"drop\")\n",
    "        print(f\"‚úÖ Binned feature generated: {feat}_bin\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è  No {feat} column in original df, skip binning\")\n",
    "\n",
    "# ===================== 3. Feature Extraction & Scorecard Matching =====================\n",
    "print(\"\\n===== Step 2: Feature Extraction & Scorecard Matching =====\")\n",
    "# 1. Features from scorecard\n",
    "scorecard_dict = scorecard2dict(scorecard)\n",
    "sc_feats = list(scorecard_dict.keys())\n",
    "print(f\"Valid features in scorecard: {sc_feats}\")\n",
    "\n",
    "# 2. Binned features (ending with _bin) in df\n",
    "df_bin_feats = [c for c in df.columns if c.endswith('_bin')]\n",
    "print(f\"Binned features (with _bin) in df: {df_bin_feats}\")\n",
    "\n",
    "# 3. Get intersection of two feature lists\n",
    "selected_feats = list(set(sc_feats) & set(df_bin_feats))\n",
    "if not selected_feats:\n",
    "    print(\"‚ö†Ô∏è  No matching features, add common binned features manually!\")\n",
    "    selected_feats = [f for f in [\"age_bin\", \"credit_amount_bin\", \"duration_bin\"] if f in df.columns]\n",
    "print(f\"Final binned features (with _bin) for scoring: {selected_feats}\")\n",
    "\n",
    "# Generate binned feature set + fill missing values\n",
    "df_binned = df[selected_feats].copy().fillna(\"Missing\")\n",
    "# Scorecard dictionary fallback processing\n",
    "for feat in selected_feats:\n",
    "    if feat not in scorecard_dict:\n",
    "        scorecard_dict[feat] = {v: 0 for v in df_binned[feat].unique()}\n",
    "    else:\n",
    "        for v in df_binned[feat].unique():\n",
    "            if v not in scorecard_dict[feat]:\n",
    "                scorecard_dict[feat][v] = 0\n",
    "print(\"‚úÖ Feature matching completed!\")\n",
    "\n",
    "# ===================== 4. Model Training =====================\n",
    "print(\"\\n===== Step 3: Logistic Regression Model Training =====\")\n",
    "model_feats = [c for c in X_train.columns if c.endswith('_woe')]\n",
    "if not model_feats:\n",
    "    raise ValueError(\"‚ùå No WOE features in X_train!\")\n",
    "lr_model = LogisticRegression(C=0.1, penalty='l1', solver='saga', random_state=42)\n",
    "lr_model.fit(X_train[model_feats], y_train)\n",
    "print(f\"‚úÖ Model training completed, using {len(model_feats)} WOE features!\")\n",
    "\n",
    "# ===================== 5. Batch Score Calculation =====================\n",
    "print(\"\\n===== Step 4: Batch Calculate Scores for Train/Test Sets =====\")\n",
    "train_scores = batch_cal_score(X_train, df_binned, scorecard_dict, selected_feats)\n",
    "test_scores = batch_cal_score(X_test, df_binned, scorecard_dict, selected_feats)\n",
    "y_train_align = y_train.reset_index(drop=True)[:len(train_scores)]\n",
    "y_test_align = y_test.reset_index(drop=True)[:len(test_scores)]\n",
    "\n",
    "# Print score distribution\n",
    "print(f\"Train set scores: {train_scores.min()} ~ {train_scores.max()}, Number of unique values: {len(np.unique(train_scores))}\")\n",
    "print(f\"Test set scores: {test_scores.min()} ~ {test_scores.max()}, Number of unique values: {len(np.unique(test_scores))}\")\n",
    "if len(np.unique(train_scores)) == 1 and train_scores[0] == 600:\n",
    "    print(\"‚ö†Ô∏è  All scores are still 600! Check if scorecard has non-zero scores!\")\n",
    "\n",
    "# ===================== 6. Basic Performance Validation =====================\n",
    "print(\"\\n===== Step 5: Model Performance Validation =====\")\n",
    "def cal_psi(train, test):\n",
    "    if len(np.unique(train)) == 1 or len(np.unique(test)) == 1:\n",
    "        return 0\n",
    "    bins = np.quantile(np.concatenate([train, test]), np.linspace(0,1,10))\n",
    "    t1, _ = np.histogram(train, bins=bins, density=True)\n",
    "    t2, _ = np.histogram(test, bins=bins, density=True)\n",
    "    t1, t2 = np.clip(t1, 1e-10, None), np.clip(t2, 1e-10, None)\n",
    "    return np.sum((t2-t1)*np.log(t2/t1))\n",
    "\n",
    "psi = cal_psi(train_scores, test_scores)\n",
    "print(f\"1. PSI: {psi:.4f} | Standard: ‚â§0.1 is qualified\")\n",
    "\n",
    "score_ana = pd.DataFrame({\"score\": test_scores, \"default\": y_test_align.values})\n",
    "score_ana[\"score_bin\"] = pd.cut(score_ana[\"score\"], 10, labels=False)\n",
    "score_seg = score_ana.groupby(\"score_bin\").agg(Avg_Score=(\"score\", \"mean\"), Default_Rate=(\"default\", \"mean\")).round(4)\n",
    "print(\"2. Score-Default Rate Monotonicity:\")\n",
    "print(score_seg)\n",
    "mono = score_seg[\"Default_Rate\"].is_monotonic_decreasing\n",
    "print(f\"   Conclusion: {'‚úÖ Qualified' if mono else '‚ö†Ô∏è Need Optimization'}\")\n",
    "\n",
    "auc = roc_auc_score(y_test_align, -1*test_scores)\n",
    "ks = calculate_ks(y_test_align, -1*test_scores)\n",
    "print(f\"3. AUC: {auc:.4f} | Standard: ‚â•0.5 is qualified\")\n",
    "print(f\"4. KS: {ks:.4f} | Standard: ‚â•0.3 is qualified\")\n",
    "if auc < 0.5:\n",
    "    print(\"‚ö†Ô∏è  AUC<0.5! Scorecard mapping is reversed, set high-risk features to negative scores!\")\n",
    "\n",
    "# ===================== 7. Model Saving =====================\n",
    "print(\"\\n===== Step 6: Model Saving =====\")\n",
    "save_path = \"./simple_risk_model/\"\n",
    "os.makedirs(save_path, exist_ok=True)\n",
    "joblib.dump(lr_model, os.path.join(save_path, \"lr_model.pkl\"))\n",
    "joblib.dump(scorecard_dict, os.path.join(save_path, \"scorecard_dict.pkl\"))\n",
    "joblib.dump(selected_feats, os.path.join(save_path, \"selected_feats.pkl\"))\n",
    "print(f\"‚úÖ Model saved successfully, files in {save_path}!\")\n",
    "\n",
    "# ===================== 8. Final Summary =====================\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéâ Step 8 of Credit Scorecard Completed!\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Core Result: Scores not all 600 = {'‚úÖ' if len(np.unique(train_scores))>1 else '‚ö†Ô∏è'}\")\n",
    "print(f\"Performance Qualified: AUC‚â•0.5 = {'‚úÖ' if auc>=0.5 else '‚ö†Ô∏è'} | KS‚â•0.3 = {'‚úÖ' if ks>=0.3 else '‚ö†Ô∏è'}\")\n",
    "print(f\"Stability Qualified: PSI‚â§0.1 = {'‚úÖ' if psi<=0.1 else '‚ö†Ô∏è'} | Monotonicity Qualified = {'‚úÖ' if mono else '‚ö†Ô∏è'}\")\n",
    "print(\"üìå Newbie Standard: Pass if scores are not all 600 + AUC‚â•0.5!\")\n",
    "print(\"=\"*50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anaconda-2025.12-py312",
   "language": "python",
   "name": "conda-env-anaconda-2025.12-py312-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
